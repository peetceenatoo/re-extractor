{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **INITIALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GuTl2RIXgsQA"
      },
      "outputs": [],
      "source": [
        "# Library for reading and writing data to and from files\n",
        "import os\n",
        "# Library for getting dictionaries from data structures\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Z5sOc92TgsLE"
      },
      "outputs": [],
      "source": [
        "# Dataset directory\n",
        "corpus_directory='../Dataset/corpus2mw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To iterate the reading procedure, get the names of the documents in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a list with the names of the documents\n",
        "texts_names = os.listdir(corpus_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **GET TOKENS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, define the special characters to be separated from each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HZYLcWMUgsNo"
      },
      "outputs": [],
      "source": [
        "# List of characters\n",
        "specialchars = [';', ':', '!', '?', '<', '>', '&', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And define the function which separates the special characters from each word, assuming they can only be before or after each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mvTiLGhygsIh"
      },
      "outputs": [],
      "source": [
        "def token(w):\n",
        "    # Init the empty list of tokens\n",
        "    res = []\n",
        "\n",
        "    # If the length is 1, add the character whatever it is\n",
        "    if len(w) == 1:\n",
        "        res.append(w)\n",
        "    \n",
        "    # Otherwise (if it's at least two characters)\n",
        "    else:\n",
        "        # If the first character is special, add it to the list and remove it from the word\n",
        "        if w[0] in specialchars:\n",
        "            res.append(w[0])\n",
        "            w = w[1:]\n",
        "        \n",
        "        # Now, if the length became 1 because of that, for the same reason as before, add the character whatever it is\n",
        "        if len(w) == 1:\n",
        "            res.append(w)\n",
        "        # Otherwise (if it's at least two characters), both if I had removed the first or not\n",
        "        # Check whether the last character is special\n",
        "        elif w[-1] in specialchars:\n",
        "            res.append(w[:-1])\n",
        "            res.append(w[-1])\n",
        "        # or not\n",
        "        else:\n",
        "            res.append(w)\n",
        "        \n",
        "    # Return the list of tokens\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each document, for each word (in each line), if either the last or the first character are specialchars, then split in multiple tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus_lens = []\n",
        "\n",
        "# For each document in the directory\n",
        "for text in texts_names:\n",
        "\n",
        "    # Init a temp empty list for the words in the current document\n",
        "    words = []\n",
        "\n",
        "    # Open the document\n",
        "    with open(corpus_directory + '/' + text, 'r', errors='ignore') as file:\n",
        "\n",
        "        # For each line\n",
        "        for line in file:\n",
        "            # For each word in the line\n",
        "            for word in line.split():\n",
        "                # Tokenize it\n",
        "                aux = token(word)\n",
        "                # And add each token to the list of words\n",
        "                for t in aux:\n",
        "                    words.append(t.lower())\n",
        "    # Then append the list of tokens for the document in the corpus list\n",
        "    corpus.append(words)\n",
        "    corpus_lens.append(len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **STOP WORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to find the stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns the extracted stop words\n",
        "def get_stop_words():\n",
        "\n",
        "    # Initialize a dictionary for ints with 0 as default value\n",
        "    neighbour_counts = defaultdict(int)\n",
        "    # Count how many neightbours each word has across the corpus\n",
        "    for doc in corpus:\n",
        "        # Increment by 1 the count for the first and last word of the current doc\n",
        "        neighbour_counts[doc[0]] += 1\n",
        "        neighbour_counts[doc[-1]] += 1\n",
        "        # From second to second last word increment by 2\n",
        "        for idx in range(1, len(doc) - 1):\n",
        "            neighbour_counts[doc[idx]] += 2\n",
        "    # And get the keys sorted by value\n",
        "    neighbour_counts = sorted(neighbour_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Remove from the neighbour_counts list the words that are special chars\n",
        "    for special in specialchars:\n",
        "        neighbour_counts = [tuple for tuple in neighbour_counts if tuple[0] != special]\n",
        "    \n",
        "    # Now look for the elbow point in the list\n",
        "    elbow_point_index = 0\n",
        "    param = 4\n",
        "    ratio = 1\n",
        "    # Iterate over the list (besides last word) to find the elbow point\n",
        "    for index in range(len(neighbour_counts) - 1 - param):\n",
        "        # Get the difference between the count for the current word and the next\n",
        "        if abs(neighbour_counts[index][1] - neighbour_counts[index + param][1]) * ratio < param:\n",
        "            elbow_point_index = index - 1\n",
        "            break\n",
        "    \n",
        "    # Get the couples corresponding to all the words up to the elbow (highest num of neighbours)\n",
        "    stop_word_counts = neighbour_counts[:elbow_point_index+1]\n",
        "    # Get the stop words\n",
        "    stop_words = [tuple[0] for tuple in stop_word_counts]\n",
        "    \n",
        "    # Return the stop words and the filtered expressions\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242\n",
            "['the', 'of', 'and', 'in', 'a', 'to', 'was', 'is', 'for', 'on', 'as', 'with', 'by', 'he', 'that', 'at', 'from', 'his', 'it', 'an', 'were', 'are', 'which', 'doc', 'this', 'also', 'be', 'or', 'has', 'had', 'first', 'their', 'one', 'but', 'its', 'after', 'not', 'new', 'they', 'who', 'have', 'two', 'her', 'she', 'been', 'other', 'when', 'during', 'there', 'into', 'all', 'time', 'more', 'only', 'may', 'most', 'school', 'years', 'would', 'over', 'some', 'out', 'such', 'national', 'up', 'him', 'later', 'about', 'used', 'where', 'between', 'world', 'then', 'city', 'many', 'can', 'made', 'three', 'while', 'state', 'year', 'under', 'known', 'part', 'these', 'united', 'than', 'university', 'second', 'being', 'became', 'no', 'american', 'season', 'before', 'both', 'team', 'states', 'through', 'however', 'war', 'including', 'early', 'born', 'film', 'them', 'against', 'well', 'family', 'since', 'will', 'until', 'history', 'area', 'series', 'high', 'south', 'album', 'name', 'number', 'group', 'people', 'work', 'de', 'north', 'district', 'played', 'called', 'each', 'following', 'music', 'i', 'released', 'several', 'county', 'life', 'â€“', 'league', 'population', 'four', 'john', 'company', 'same', 'house', 'now', 'won', 'government', 'career', 'game', 'any', 'use', 'so', 'if', '1', 'town', 'international', 'end', 'system', 'found', 'september', 'because', 'located', 'general', 'march', 'west', 'home', 'began', 'age', 'july', 'june', 'august', 'october', 'station', 'named', 'january', '2010', 'around', 'member', 'day', 'april', 'river', 'york', 'band', 'british', '2008', 'place', 'public', '2011', 'along', 'line', 'based', 'another', 'college', 'due', 'like', 'song', 'did', 'back', 'held', '2009', 'could', 'former', 'best', 'local', 'although', 'received', 'club', 'church', 'very', 'november', 'took', 'football', 'east', 'december', 'members', 'century', 'show', '2012', 'major', 'village', 'service', '2', 'small', 'single', 'final', 'include', '2007', '2006', 'long', 'within', 'party', 'own', 'built', 'games', '2013', 'last', 'death', 'still', 'five', 'set', 'order', 'february']\n"
          ]
        }
      ],
      "source": [
        "# Get stop words\n",
        "stop_words = get_stop_words()\n",
        "print(len(stop_words))\n",
        "print(stop_words)\n",
        "\n",
        "# And write them all on file\n",
        "with open('../Output/stop_words.txt', 'w') as f:\n",
        "    for word in stop_words:\n",
        "        f.write(\"\\\"\")\n",
        "        f.write(word)\n",
        "        f.write(\"\\\"\")\n",
        "        f.write(', ')\n",
        "del corpus"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
