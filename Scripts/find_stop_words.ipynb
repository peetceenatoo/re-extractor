{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **INITIALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuTl2RIXgsQA"
      },
      "outputs": [],
      "source": [
        "# Library for reading and writing data to and from files\n",
        "import os\n",
        "# Library for getting dictionaries from data structures\n",
        "from collections import defaultdict\n",
        "# Library for reading and writing data to and from files\n",
        "import pandas as pd\n",
        "# Library to save the plots as html files\n",
        "import plotly.io as pio\n",
        "# Library which contains the graph objects that will be used to create the plots\n",
        "import plotly.graph_objs as go\n",
        "# Library to perform mathematical operations\n",
        "import numpy as np\n",
        "# Library to interpolate the data\n",
        "from scipy.interpolate import griddata\n",
        "# Library to do the histograms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sOc92TgsLE"
      },
      "outputs": [],
      "source": [
        "# Dataset directory\n",
        "corpus_directory='../Dataset/corpus2mw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To iterate the reading procedure, get the names of the documents in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a list with the names of the documents\n",
        "texts_names = os.listdir(corpus_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **GET TOKENS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, define the special characters to be separated from each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZYLcWMUgsNo"
      },
      "outputs": [],
      "source": [
        "# List of characters\n",
        "specialchars = [';', ':', '!', '?', '<', '>', '&', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And define the function which separates the special characters from each word, assuming they can only be before or after each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvTiLGhygsIh"
      },
      "outputs": [],
      "source": [
        "def token(w):\n",
        "    # Init the empty list of tokens\n",
        "    res = []\n",
        "\n",
        "    # If the length is 1, add the character whatever it is\n",
        "    if len(w) == 1:\n",
        "        res.append(w)\n",
        "    \n",
        "    # Otherwise (if it's at least two characters)\n",
        "    else:\n",
        "        # If the first character is special, add it to the list and remove it from the word\n",
        "        if w[0] in specialchars:\n",
        "            res.append(w[0])\n",
        "            w = w[1:]\n",
        "        \n",
        "        # Now, if the length became 1 because of that, for the same reason as before, add the character whatever it is\n",
        "        if len(w) == 1:\n",
        "            res.append(w)\n",
        "        # Otherwise (if it's at least two characters), both if I had removed the first or not\n",
        "        # Check whether the last character is special\n",
        "        elif w[-1] in specialchars:\n",
        "            res.append(w[:-1])\n",
        "            res.append(w[-1])\n",
        "        # or not\n",
        "        else:\n",
        "            res.append(w)\n",
        "        \n",
        "    # Return the list of tokens\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each document, for each word (in each line), if either the last or the first character are specialchars, then split in multiple tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus_lens = []\n",
        "\n",
        "# For each document in the directory\n",
        "for text in texts_names:\n",
        "\n",
        "    # Init a temp empty list for the words in the current document\n",
        "    words = []\n",
        "\n",
        "    # Open the document\n",
        "    with open(corpus_directory + '/' + text, 'r', errors='ignore') as file:\n",
        "\n",
        "        # For each line\n",
        "        for line in file:\n",
        "            # For each word in the line\n",
        "            for word in line.split():\n",
        "                # Tokenize it\n",
        "                aux = token(word)\n",
        "                # And add each token to the list of words\n",
        "                for t in aux:\n",
        "                    words.append(t.lower())\n",
        "    # Then append the list of tokens for the document in the corpus list\n",
        "    corpus.append(words)\n",
        "    corpus_lens.append(len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **STOP WORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function which returns the number of distinct neighbours for each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate neighbour counts\n",
        "def calculate_neighbour_counts():\n",
        "    # Initialize a dictionary for sets\n",
        "    neighbour_counts_dict = defaultdict(set)\n",
        "\n",
        "    # For each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # For each word in the document\n",
        "        for idx in range(len(doc)):\n",
        "            # If the word is not the first word, add the previous word to its set of neighbours\n",
        "            if idx > 0:\n",
        "                neighbour_counts_dict[doc[idx]].add(doc[idx - 1])\n",
        "            # If the word is not the last word, add the next word to its set of neighbours\n",
        "            if idx < len(doc) - 1:\n",
        "                neighbour_counts_dict[doc[idx]].add(doc[idx + 1])\n",
        "\n",
        "    # Convert the sets of neighbours to counts of unique neighbours\n",
        "    for word, neighbours in neighbour_counts_dict.items():\n",
        "        neighbour_counts_dict[word] = len(neighbours)\n",
        "\n",
        "    # Sort the words by their counts of unique neighbours\n",
        "    neighbour_counts_list = sorted(neighbour_counts_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Remove from the neighbour_counts list the words that are special chars\n",
        "    for special in specialchars:\n",
        "        neighbour_counts_list = [item for item in neighbour_counts_list if item[0] != special]\n",
        "\n",
        "    return neighbour_counts_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to find the stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns the extracted stop words\n",
        "def get_stop_words(param=4, d=1):\n",
        "\n",
        "    # Get the neighbours\n",
        "    neighbour_counts = calculate_neighbour_counts()\n",
        "    \n",
        "    # Now look for the elbow point in the list\n",
        "    elbow_point_index = 0\n",
        "    # Iterate over the list (besides last word) to find the elbow point\n",
        "    for index in range(len(neighbour_counts) - 1 - param):\n",
        "        # Get the difference between the count for the current word and the next\n",
        "        if abs(neighbour_counts[index][1] - neighbour_counts[index + param][1]) * d < param:\n",
        "            elbow_point_index = index - 1\n",
        "            break\n",
        "    \n",
        "    # Get the couples corresponding to all the words up to the elbow (highest num of neighbours)\n",
        "    stop_word_counts = neighbour_counts[:elbow_point_index+1]\n",
        "    # Get the stop words\n",
        "    stop_words = [tuple[0] for tuple in stop_word_counts]\n",
        "    \n",
        "    # Return the stop words and the filtered expressions\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(get_stop_words())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store the neighbours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nei = calculate_neighbour_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute the stop words for various settings of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the ranges for param and d\n",
        "param_values = range(1, 7)\n",
        "d_values = [1/3, 1/2, 1, 2, 3]\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over all combinations of param and d\n",
        "for param in param_values:\n",
        "    for d in d_values:\n",
        "        # Get the stop words for the current combination of param and d\n",
        "        stop_words = get_stop_words(param, d)\n",
        "        # Append the result as a tuple (param, d, number of stop words)\n",
        "        results.append((param, d, len(stop_words)))\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df = pd.DataFrame(results, columns=['param', 'd', 'num_stop_words'])\n",
        "\n",
        "# Pivot the DataFrame to create the desired table\n",
        "pivot_df = df.pivot(index='d', columns='param', values='num_stop_words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get plot of the surface in a variable and store it in a HTML file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define x, y, z values\n",
        "\n",
        "# Convert param_values into np.arange from range\n",
        "param = np.array(param_values)\n",
        "d = np.array(d_values)\n",
        "\n",
        "stop_words = np.array([\n",
        "    [26, 76, 83, 94, 94, 119],\n",
        "    [40, 84, 94, 94, 121, 121],\n",
        "    [61, 84, 123, 154, 154, 154],\n",
        "    [61, 224, 176, 250, 176, 250],\n",
        "    [61, 224, 284, 250, 284, 284]\n",
        "])\n",
        "\n",
        "# Create meshgrid for x and y\n",
        "X, Y = np.meshgrid(param, d)\n",
        "\n",
        "# Interpolate z values to complete grid\n",
        "X_new = np.linspace(param.min(), param.max(), 100)\n",
        "Y_new = np.linspace(d.min(), d.max(), 100)\n",
        "X_new, Y_new = np.meshgrid(X_new, Y_new)\n",
        "Z_new = griddata((X.flatten(), Y.flatten()), stop_words.flatten(), (X_new, Y_new), method='cubic')\n",
        "\n",
        "# Scatter plot\n",
        "scatter = go.Scatter3d(\n",
        "    x=X.flatten(),\n",
        "    y=Y.flatten(),\n",
        "    z=stop_words.flatten(),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=4,\n",
        "        color='rgb(23, 190, 207)',\n",
        "        opacity=0.8\n",
        "    )\n",
        ")\n",
        "\n",
        "# Surface plot\n",
        "surface = go.Surface(\n",
        "    x=X_new,\n",
        "    y=Y_new,\n",
        "    z=Z_new,\n",
        "    colorscale='Viridis',\n",
        ")\n",
        "\n",
        "# Add point (4,1,F(4,1))\n",
        "point = go.Scatter3d(\n",
        "    x=[4],\n",
        "    y=[1],\n",
        "    z=[154],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=10,\n",
        "        color='red',\n",
        "        opacity=0.8\n",
        "    )\n",
        ")\n",
        "\n",
        "# Layout\n",
        "layout = go.Layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(title='param'),\n",
        "        yaxis=dict(title='d'),\n",
        "        zaxis=dict(title='Number of stop words'),\n",
        "        aspectratio=dict(x=1, y=1, z=0.7),\n",
        "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create figure\n",
        "fig = go.Figure(data=[scatter, surface, point], layout=layout)\n",
        "\n",
        "# And store it in HTML\n",
        "pio.write_html(fig, '../Output/3d_plot_surface.html')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot histogram for top 10 stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the data\n",
        "words = ['the', 'and', 'of', 'in', 'to', 'a', 'was', 'is', 'for', 'by']\n",
        "frequencies = [28312, 26112, 20895, 16628, 12515, 11378, 8664, 8165, 7703, 7589]\n",
        "\n",
        "# Create the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words, frequencies, color='darkgreen')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Neighbours')\n",
        "plt.title('Top 10 stop words')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the histogram for the first 500 ranked words by number of neighbours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get first 500 ranked words by neighbours\n",
        "values = [value for key, value in nei[:500]]\n",
        "\n",
        "# And plot the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(values)), values, color='skyblue')\n",
        "plt.xlabel('Words rank')\n",
        "plt.ylabel('Neighbours')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
