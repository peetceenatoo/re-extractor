{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **INITIALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GuTl2RIXgsQA"
      },
      "outputs": [],
      "source": [
        "# Library for reading and writing data to and from files\n",
        "import os\n",
        "# Library for numerical computing\n",
        "import numpy as np\n",
        "# Library for mathematical functions\n",
        "import math\n",
        "# Library for getting dictionaries from data structures\n",
        "from collections import defaultdict\n",
        "# Library to get fit models\n",
        "from scipy.optimize import curve_fit\n",
        "# Library to plot histograms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Z5sOc92TgsLE"
      },
      "outputs": [],
      "source": [
        "# Dataset directory\n",
        "corpus_directory='../Dataset/corpus2mw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(55555555)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **GET TOKENS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, define the special characters to be separated from each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HZYLcWMUgsNo"
      },
      "outputs": [],
      "source": [
        "# List of characters\n",
        "specialchars = [';', ':', '!', '?', '<', '>', '&', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-', '_', '+', '*', '#', '@']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And define the function which separates the special characters from each word, assuming they can only be before or after each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mvTiLGhygsIh"
      },
      "outputs": [],
      "source": [
        "def token(w):\n",
        "    # Init the empty list of tokens\n",
        "    res = []\n",
        "\n",
        "    # If the length is 1, add the character whatever it is\n",
        "    if len(w) == 1:\n",
        "        res.append(w)\n",
        "    \n",
        "    # Otherwise (if it's at least two characters)\n",
        "    else:\n",
        "        # If the first character is special, add it to the list and remove it from the word\n",
        "        if w[0] in specialchars:\n",
        "            res.append(w[0])\n",
        "            w = w[1:]\n",
        "        \n",
        "        # Now, if the length became 1 because of that, for the same reason as before, add the character whatever it is\n",
        "        if len(w) == 1:\n",
        "            res.append(w)\n",
        "        # Otherwise (if it's at least two characters), both if I had removed the first or not\n",
        "        # Check whether the last character is special\n",
        "        elif w[-1] in specialchars:\n",
        "            res.append(w[:-1])\n",
        "            res.append(w[-1])\n",
        "        # or not\n",
        "        else:\n",
        "            res.append(w)\n",
        "        \n",
        "    # Return the list of tokens\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each word (in each line) in the documents, if either the last or the first character are specialchars, then split in multiple tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts_names = ['fil_1']\n",
        "corpus = []\n",
        "corpus_lens = []\n",
        "\n",
        "# For each document in the directory\n",
        "for text in texts_names:\n",
        "\n",
        "    # Init a temp empty list for the words in the current document\n",
        "    words = []\n",
        "\n",
        "    # Open the document\n",
        "    with open(corpus_directory + '/' + text, 'r', errors='ignore') as file:\n",
        "\n",
        "        # For each line\n",
        "        for line in file:\n",
        "            # For each word in the line\n",
        "            for word in line.split():\n",
        "                # Tokenize it\n",
        "                aux = token(word)\n",
        "                # And add each token to the list of words\n",
        "                for t in aux:\n",
        "                    words.append(t.lower())\n",
        "    # Then append the list of tokens for the document in the corpus list\n",
        "    corpus.append(words)\n",
        "    corpus_lens.append(len(words))\n",
        "\n",
        "corpus_len = len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way, the corpus is a list of documents, which are lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# And compute the overall number of words\n",
        "num_of_words_in_corpus = sum(corpus_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to get a list of words from a string with words separated with a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "l88ONep-gr5i"
      },
      "outputs": [],
      "source": [
        "# Given a list of strings, it returns a string\n",
        "# in which the substrings will be separated by ' '\n",
        "def list_to_str(strings):\n",
        "    # Init res as an empty string\n",
        "    res = \"\"\n",
        "    # For each character/string in the list\n",
        "    for i in range(len(strings)):\n",
        "        # Concatenate the string plus a space to res\n",
        "        res += strings[i] + ' '\n",
        "    # Then return everything besides the last space\n",
        "    return res[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to do the opposite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a string which contains substrings separated by ' ',\n",
        "# it returns a list of words\n",
        "def str_to_list(s):\n",
        "    # Init res as an empty list\n",
        "    res = []\n",
        "    # Split the string by ' ' and for each substring\n",
        "    for word in s.split():\n",
        "        # Append the substring to res\n",
        "        res.append(word)\n",
        "    # Return the list of substrings\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Declare a function which returns a list of n dictionaries, one for each n up to a fixed max, which contain the information about the frequencies for each n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n dictionaries, one for all the possible n-grams, with n in [1, max]\n",
        "# Each n-dictionary will map each n-gram to a list with the absolute frequency in [0]\n",
        "# and then the index of the documents in which it was found once at least,\n",
        "# followed by the relative frequency (e.g. [25, 1, 20, 2, 1, 3, 4])\n",
        "def create_list_of_dict_global(max):\n",
        "    # Init an empty list of dictionaries\n",
        "    list_dict=[]\n",
        "\n",
        "    # For each n in [1, max], append an empty dictionary to the list\n",
        "    for n in range(max):\n",
        "        list_dict.append({})\n",
        "\n",
        "    # For each index of a document in the corpus\n",
        "    for i in range(corpus_len):\n",
        "        # For each index of a token in the document\n",
        "        for t in range(len(corpus[i])):\n",
        "            # For each n in [1, max]\n",
        "            for n in range(1, len(list_dict)+1):\n",
        "                # If the document is not over (there is still space for an n-gram)\n",
        "                if ( t + n ) <= len(corpus[i]) :\n",
        "                    # If the n-gram is not yet in the n-dictionary\n",
        "                    if not ( list_to_str(corpus[i][t : t+n]) in list_dict[n-1].keys() ) :\n",
        "                        # Associate to the new n-gram an empty list\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])] = []\n",
        "                        # Set the frequency to 1 in position [0]\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                        # And then append the index of the current document\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                        # and set the relative frequency to 1\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                    else:\n",
        "                        # Add one to the frequency of the n-gram\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])][0] += 1\n",
        "                        # And if the last document in which this n-gram was found\n",
        "                        # is still the current one \n",
        "                        if list_dict[n-1][list_to_str(corpus[i][t : t+n])][-2] == i :\n",
        "                            # Just increment the relative frequency\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])][-1] += 1\n",
        "                        # Otherwise\n",
        "                        else:\n",
        "                            # Append the new (current) document\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                            # and set the relative frequency to 1\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "    # Then return the list of dictionaries\n",
        "    return list_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pick a max value for n. Remember that if you care about the 7-grams you need the 8-grams to be computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary with the above function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "409mbD-jgrzf",
        "outputId": "ddba2b7f-67d6-4bbd-b564-1553d9de5bf7"
      },
      "outputs": [],
      "source": [
        "list_of_ngrams_info_dictionaries = create_list_of_dict_global(max_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **STOP WORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to find the stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns the extracted stop words\n",
        "def get_stop_words(param=4, d=1):\n",
        "\n",
        "    # Initialize a dictionary for sets\n",
        "    neighbour_counts = defaultdict(set)\n",
        "\n",
        "    # For each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # For each word in the document\n",
        "        for idx in range(len(doc)):\n",
        "            # If the word is not the first word, add the previous word to its set of neighbours\n",
        "            if idx > 0:\n",
        "                neighbour_counts[doc[idx]].add(doc[idx - 1])\n",
        "            # If the word is not the last word, add the next word to its set of neighbours\n",
        "            if idx < len(doc) - 1:\n",
        "                neighbour_counts[doc[idx]].add(doc[idx + 1])\n",
        "\n",
        "    # Convert the sets of neighbours to counts of unique neighbours\n",
        "    for word, neighbours in neighbour_counts.items():\n",
        "        neighbour_counts[word] = len(neighbours)\n",
        "\n",
        "    # Sort the words by their counts of unique neighbours\n",
        "    neighbour_counts = sorted(neighbour_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Remove from the neighbour_counts list the words that are special chars\n",
        "    for special in specialchars:\n",
        "        neighbour_counts = [tuple for tuple in neighbour_counts if tuple[0] != special]\n",
        "    \n",
        "    # Now look for the elbow point in the list\n",
        "    elbow_point_index = 0\n",
        "    # Iterate over the list (besides last word) to find the elbow point\n",
        "    for index in range(len(neighbour_counts) - 1 - param):\n",
        "        # Get the difference between the count for the current word and the next\n",
        "        if abs(neighbour_counts[index][1] - neighbour_counts[index + param][1]) * d < param:\n",
        "            elbow_point_index = index - 1\n",
        "            break\n",
        "    \n",
        "    # Get the couples corresponding to all the words up to the elbow (highest num of neighbours)\n",
        "    stop_word_counts = neighbour_counts[:elbow_point_index+1]\n",
        "    # Get the stop words\n",
        "    stop_words = [tuple[0] for tuple in stop_word_counts]\n",
        "    \n",
        "    # Return the stop words and the filtered expressions\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get stop words\n",
        "stop_words = get_stop_words()\n",
        "del corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **EXPLORING GLUES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function which computes the glues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if n_max is 8, compute glues for uni-grams up to 7-grams\n",
        "def compute_glues(gluename):\n",
        "    # Initialize an empty list for storing the glue dictionaries\n",
        "    glue_dicts = []\n",
        "\n",
        "    # If n_max is 8, for each n in [1, 8)\n",
        "    for n in range(1, len(list_of_ngrams_info_dictionaries)):\n",
        "\n",
        "        # Initialize the glue dictionary as a dictionary with the same keys as list_of_ngrams_info_dictionaries[n-1] associating 0 to each\n",
        "        glue_dict = dict.fromkeys(list_of_ngrams_info_dictionaries[n - 1], 0)\n",
        "\n",
        "        # For each n-gram with its frequencies list\n",
        "        for key, value in list_of_ngrams_info_dictionaries[n - 1].items():\n",
        "            # Only if n is at least 2 or if it is a monogram of length 3\n",
        "            if ( n != 1 ) or ( len(key) >= 3 ):\n",
        "                # Store the absolute frequency for the n-gram\n",
        "                abs_freq = value[0]\n",
        "\n",
        "                # If n is at least 2\n",
        "                if n != 1:\n",
        "                    # Convert the n-gram into a list instead of a string\n",
        "                    key_list = str_to_list(key)\n",
        "\n",
        "                    # Initialize the sum to zero\n",
        "                    s = 0\n",
        "                    \n",
        "                    # Compute the right GLUE coefficient\n",
        "\n",
        "                    # Do Dice\n",
        "                    if gluename == 'Dice':\n",
        "\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = list_of_ngrams_info_dictionaries[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = list_of_ngrams_info_dictionaries[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 + f2) / (n - 1)\n",
        "\n",
        "                        gl = (2 * abs_freq) / s\n",
        "\n",
        "                    # Do SCP\n",
        "                    elif gluename == 'SCP':\n",
        "\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = list_of_ngrams_info_dictionaries[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = list_of_ngrams_info_dictionaries[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 * f2) / (n - 1)\n",
        "\n",
        "                        gl = (abs_freq**2) / s\n",
        "\n",
        "                    # Compute MI\n",
        "                    elif gluename == 'MI':\n",
        "\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = list_of_ngrams_info_dictionaries[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = list_of_ngrams_info_dictionaries[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 * f2) / (n - 1)\n",
        "\n",
        "                        gl = math.log(abs_freq * num_of_words_in_corpus / s)\n",
        "\n",
        "                    else:\n",
        "                        gl = 0\n",
        "\n",
        "                    # Add the glue to the list of the current n-gram\n",
        "                    glue_dict[key] = gl\n",
        "\n",
        "            # Otherwise if we have a monogram which is not long enough (3 letters at least)\n",
        "            else:\n",
        "                # Remove the list related to such n-gram from the glue dictionary\n",
        "                glue_dict.pop(key)\n",
        "\n",
        "        # Append the n-th glue dictionary to the list of glue dictionaries\n",
        "        glue_dicts.append(glue_dict)\n",
        "\n",
        "    # Return the list of glue dictionaries\n",
        "    return glue_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the glues for each n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing GLUEs with SCP...\n",
            "Computing GLUEs with Dice...\n",
            "Computing GLUEs with MI...\n"
          ]
        }
      ],
      "source": [
        "print(\"Computing GLUEs with SCP...\")\n",
        "SCP_glues = compute_glues('SCP')\n",
        "print(\"Computing GLUEs with Dice...\")\n",
        "dice_glues = compute_glues('Dice')\n",
        "print(\"Computing GLUEs with MI...\")\n",
        "MI_glues = compute_glues('MI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **REGULAR EXPRESSIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute a list with n dictionaries, each associating to each n-gram all the (n+1)-grams which have one more word on the left/right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of dictionaries, one for each value of n\n",
        "fathers = []\n",
        "\n",
        "# Given max_n = 8, notice n in [1, 8) obviously, seaching among those the fathers of the children for [0, 7) that are from uni-grams to 7-grams\n",
        "for n in range(1, max_n):\n",
        "\n",
        "    # Get a dictionary with all the keys of the n-th dictionary in list_of_ngrams_info_dictionaries, and empty lists as values\n",
        "    f = {key: [] for key in dict(list_of_ngrams_info_dictionaries[n - 1]).keys()}\n",
        "\n",
        "    # For each (n+1)-gram (dictionary when n=2 actually contains 3-grams)\n",
        "    for key, value in list_of_ngrams_info_dictionaries[n].items():\n",
        "        # Get the (n+1)-gram as list of words\n",
        "        key_list = str_to_list(key)\n",
        "        # Get the two sub-n-grams of the (n+1)-gram\n",
        "        subkey1 = list_to_str(key_list[1:])\n",
        "        subkey2 = list_to_str(key_list[:-1])\n",
        "        # Add them in the temp dictionary\n",
        "        f[subkey1].append(key)\n",
        "        f[subkey2].append(key)\n",
        "        \n",
        "    # And finally append the dictionary to the list fathers\n",
        "    fathers.append(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define the function which returns a dictionary containing all the Multiword Expressions (REs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auxiliary function (for readability) which checks whether the \"key_string\" n-gram is a RE\n",
        "def check_MWE(key_string, glues, REglues):\n",
        "    # Get n as the number of spaces in key_string + 1\n",
        "    n = key_string.count(' ')\n",
        "    \n",
        "    # Get the glue for the n-gram\n",
        "    glue = glues[n][key_string]\n",
        "\n",
        "    # Get key_string as list to easily remove the first/last word\n",
        "    key_list = str_to_list(key_string)\n",
        "\n",
        "    # Get the set of glues for (n-1)-grams\n",
        "    omega_n_minus = set()\n",
        "    # If it's a 2-gram do not check for the glues of 1-grams\n",
        "    if n > 1:\n",
        "        omega_n_minus.add(glues[n - 1][list_to_str(key_list[1:])])\n",
        "        omega_n_minus.add(glues[n - 1][list_to_str(key_list[:-1])])\n",
        "\n",
        "    # Get the set of glues associated to all the super-n-grams\n",
        "    omega_n_plus = set([glues[n + 1][fath] for fath in fathers[n][key_string]])\n",
        "\n",
        "    # Get the max glue among all the sub-grams and super-grams\n",
        "    max_glue = max(omega_n_minus.union(omega_n_plus))\n",
        "    # And if it's better, store it as a new RE, associated with its glue\n",
        "    if glue > max_glue:\n",
        "        REglues[key_string] = glues[n][key_string]\n",
        "\n",
        "# Returns two dictionaries, only containing REs as keys\n",
        "def find_RE(glues):\n",
        "    # Init the new dictionary containing only the REs mapped to their glue\n",
        "    REglues = {}\n",
        "\n",
        "    # For each n in [1, max_n)\n",
        "    for n in range(1, len(glues)-1):\n",
        "        # For each n-gram\n",
        "        for key, _ in glues[n].items() :\n",
        "            # Process the n-gram to decide whether it is a RE\n",
        "            check_MWE(key, glues, REglues)\n",
        "\n",
        "    # Return the two dictionaries\n",
        "    return REglues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then compute the REs information for each glue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding REs with SCP...\n",
            "Finding REs with Dice...\n",
            "Finding REs with MI...\n"
          ]
        }
      ],
      "source": [
        "print(\"Finding REs with SCP...\")\n",
        "RE_SCP_glues = find_RE(SCP_glues)\n",
        "print(\"Finding REs with Dice...\")\n",
        "RE_dice_glues = find_RE(dice_glues)\n",
        "print(\"Finding REs with MI...\")\n",
        "RE_MI_glues = find_RE(MI_glues)\n",
        "\n",
        "del SCP_glues\n",
        "del dice_glues\n",
        "del MI_glues\n",
        "del fathers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **FILTERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to delete REs containing special characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gets a string and returns false if must be deleted\n",
        "def no_special(key_string):\n",
        "    for i in range(len(key_string)):\n",
        "        if (key_string[i] in specialchars):\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to delete REs contained in one only document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gets a string and returns false if must be deleted\n",
        "def more_documents(key_list):\n",
        "    if (list_of_ngrams_info_dictionaries[len(str_to_list(key_list)) - 1][key_list][0] > 1):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remember REs datastructures are now just dictionaries, not lists of dictionaries\n",
        "RE_SCP_glues_filtered = {}\n",
        "RE_dice_glues_filtered = {}\n",
        "RE_MI_glues_filtered = {}\n",
        "\n",
        "# Iterate through REs and filter SCP REs\n",
        "for key, value in RE_SCP_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_SCP_glues_filtered[key] = value\n",
        "\n",
        "# Iterate through REs and filter Dice REs\n",
        "for key, value in RE_dice_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_dice_glues_filtered[key] = value\n",
        "\n",
        "# Iterate through REs and filter MI REs\n",
        "for key, value in RE_MI_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_MI_glues_filtered[key] = value\n",
        "        \n",
        "del RE_SCP_glues\n",
        "del RE_dice_glues\n",
        "del RE_MI_glues\n",
        "\n",
        "# All the REs in the corresponding list of n-grams if the first and the last word are not stop-words\n",
        "RE_SCP_glues_filtered = {re: glue for re, glue in RE_SCP_glues_filtered.items() if str_to_list(re)[0] not in stop_words and str_to_list(re)[-1] not in stop_words}\n",
        "RE_dice_glues_filtered = {re: glue for re, glue in RE_dice_glues_filtered.items() if str_to_list(re)[0] not in stop_words and str_to_list(re)[-1] not in stop_words}\n",
        "RE_MI_glues_filtered = {re: glue for re, glue in RE_MI_glues_filtered.items() if str_to_list(re)[0] not in stop_words and str_to_list(re)[-1] not in stop_words}\n",
        "\n",
        "del stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally get the REs for each doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_per_doc(REs_glues):\n",
        "\n",
        "    #### Initialize dictionaries for unigrams and n-grams matches\n",
        "    REs_per_doc = {}\n",
        "\n",
        "    # Init lists for each word\n",
        "    for k in range(corpus_len):\n",
        "        REs_per_doc['doc' + str(k)] = []\n",
        "\n",
        "    #### Populate the REs match dictionary\n",
        "    for key, _ in REs_glues.items():\n",
        "        n = key.count(' ')\n",
        "        for index in range(1, len(list_of_ngrams_info_dictionaries[n][key]), 2):\n",
        "            REs_per_doc['doc' + str(list_of_ngrams_info_dictionaries[n][key][index])].append(key)\n",
        "\n",
        "    return REs_per_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "REs_per_doc_SCP = get_per_doc(RE_SCP_glues_filtered)\n",
        "REs_per_doc_dice = get_per_doc(RE_dice_glues_filtered)\n",
        "REs_per_doc_MI = get_per_doc(RE_MI_glues_filtered)\n",
        "\n",
        "# Print\n",
        "with open('../Output/REs_per_doc.txt', 'w') as f:\n",
        "\n",
        "    # Print using SCP\n",
        "    for key in REs_per_doc_SCP.keys():\n",
        "\n",
        "\n",
        "        index = int(key.lstrip('doc'))\n",
        "        f.write(texts_names[index])\n",
        "        f.write(': ')\n",
        "        f.write('\\n')\n",
        "        f.write('SCP - ')\n",
        "        for v in REs_per_doc_SCP[key]:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "\n",
        "        f.write('Dice - ')\n",
        "        for v in REs_per_doc_dice[key]:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "\n",
        "        f.write('Mi - ')\n",
        "        for v in REs_per_doc_MI[key]:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "\n",
        "    f.write('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
