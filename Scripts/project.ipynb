{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **INITIALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuTl2RIXgsQA"
      },
      "outputs": [],
      "source": [
        "# Library for reading and writing data to and from files\n",
        "import os\n",
        "# Library for numerical computing\n",
        "import numpy as np\n",
        "# Library for mathematical functions\n",
        "import math\n",
        "# Library for getting dictionaries from data structures\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sOc92TgsLE"
      },
      "outputs": [],
      "source": [
        "# Dataset directory\n",
        "corpus_directory='../Dataset/corpus2mw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To iterate the reading procedure, get the names of the documents in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a list with the names of the documents\n",
        "texts_names = os.listdir(corpus_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(55555555)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **GET TOKENS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, define the special characters to be separated from each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZYLcWMUgsNo"
      },
      "outputs": [],
      "source": [
        "# List of characters\n",
        "specialchars = [';', ':', '!', '?', '<', '>', '&', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And define the function which separates the special characters from each word, assuming they can only be before or after each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvTiLGhygsIh"
      },
      "outputs": [],
      "source": [
        "def token(w):\n",
        "    # Init the empty list of tokens\n",
        "    res = []\n",
        "\n",
        "    # If the length is 1, add the character whatever it is\n",
        "    if len(w) == 1:\n",
        "        res.append(w)\n",
        "    \n",
        "    # Otherwise (if it's at least two characters)\n",
        "    else:\n",
        "        # If the first character is special, add it to the list and remove it from the word\n",
        "        if w[0] in specialchars:\n",
        "            res.append(w[0])\n",
        "            w = w[1:]\n",
        "        \n",
        "        # Now, if the length became 1 because of that, for the same reason as before, add the character whatever it is\n",
        "        if len(w) == 1:\n",
        "            res.append(w)\n",
        "        # Otherwise (if it's at least two characters), both if I had removed the first or not\n",
        "        # Check whether the last character is special\n",
        "        elif w[-1] in specialchars:\n",
        "            res.append(w[:-1])\n",
        "            res.append(w[-1])\n",
        "        # or not\n",
        "        else:\n",
        "            res.append(w)\n",
        "        \n",
        "    # Return the list of tokens\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each document, for each word (in each line), if either the last or the first character are specialchars, then split in multiple tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "# For each document in the directory\n",
        "for text in texts_names:\n",
        "\n",
        "    # Init a temp empty list for the words in the current document\n",
        "    words = []\n",
        "\n",
        "    # Open the document\n",
        "    with open(corpus_directory + '/' + text, 'r', errors='ignore') as file:\n",
        "\n",
        "        # For each line\n",
        "        for line in file:\n",
        "            # For each word in the line\n",
        "            for word in line.split():\n",
        "                # Tokenize it\n",
        "                aux = token(word)\n",
        "                # And add each token to the list of words\n",
        "                for t in aux:\n",
        "                    words.append(t)\n",
        "    # Then append the list of tokens for the document in the corpus list\n",
        "    corpus.append(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way, the corpus is a list of documents, which are lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DfPdppoOYO7",
        "outputId": "948487ce-c872-4b4c-fce7-97fd93d5f5bd"
      },
      "outputs": [],
      "source": [
        "# Visualize first 10 tokens in the second document corpus[1]\n",
        "print(corpus[1][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to get a list of words from a string with words separated with a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l88ONep-gr5i"
      },
      "outputs": [],
      "source": [
        "# Given a list of strings, it returns a string\n",
        "# in which the substrings will be separated by ' '\n",
        "def list_to_str(strings):\n",
        "    # Init res as an empty string\n",
        "    res = \"\"\n",
        "    # For each character/string in the list\n",
        "    for i in range(len(strings)):\n",
        "        # Concatenate the string plus a space to res\n",
        "        res += strings[i] + ' '\n",
        "    # Then return everything besides the last space\n",
        "    return res[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to do the opposite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a string which contains substrings separated by ' ',\n",
        "# it returns a list of words\n",
        "def str_to_list(s):\n",
        "    # Init res as an empty list\n",
        "    res = []\n",
        "    # Split the string by ' ' and for each substring\n",
        "    for word in s.split():\n",
        "        # Append the substring to res\n",
        "        res.append(word)\n",
        "    # Return the list of substrings\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Declare a function which returns a list of n dictionaries, one for each n up to a fixed max, which contain the information about the frequencies for each n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n dictionaries, one for all the possible n-grams, with n in [1, max]\n",
        "# Each n-dictionary will map each n-gram to a list with the absolute frequency in [0]\n",
        "# and then the index of the documents in which it was found once at least,\n",
        "# followed by the relative frequency (e.g. [25, 1, 20, 2, 1, 3, 4])\n",
        "def create_list_of_dict_global(max):\n",
        "    # Init an empty list of dictionaries\n",
        "    list_dict=[]\n",
        "\n",
        "    # For each n in [1, max], append an empty dictionary to the list\n",
        "    for n in range(max):\n",
        "        list_dict.append({})\n",
        "\n",
        "    # For each index of a document in the corpus\n",
        "    for i in range(len(corpus)):\n",
        "        # For each index of a token in the document\n",
        "        for t in range(len(corpus[i])):\n",
        "            # For each n in [1, max]\n",
        "            for n in range(1, len(list_dict)+1):\n",
        "                # If the document is not over (there is still space for an n-gram)\n",
        "                if ( t + n ) <= len(corpus[i]) :\n",
        "                    # If the n-gram is not yet in the n-dictionary\n",
        "                    if not ( list_to_str(corpus[i][t : t+n]) in list_dict[n-1].keys() ) :\n",
        "                        # Associate to the new n-gram an empty list\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])] = []\n",
        "                        # Set the frequency to 1 in position [0]\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                        # And then append the index of the current document\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                        # and set the relative frequency to 1\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                    else:\n",
        "                        # Add one to the frequency of the n-gram\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])][0] += 1\n",
        "                        # And if the last document in which this n-gram was found\n",
        "                        # is the current one\n",
        "                        if list_dict[n-1][list_to_str(corpus[i][t : t+n])][-2] == i :\n",
        "                            # Just increment the relative frequency\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])][-1] += 1\n",
        "                        # Otherwise\n",
        "                        else:\n",
        "                            # Append the new (current) document\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                            # and set the relative frequency to 1\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "    # Then return the list of dictionaries\n",
        "    return list_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to find all the indices at which a given n-gram occurs in a given document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n dictionaries, one for all the possible n-grams, with n in [1, max]\n",
        "def find_indices_ngram_doc(ngram_string, docnum):\n",
        "    # Init an empty list for the indices\n",
        "    indices = []\n",
        "    # Get the document as a list of tokens \n",
        "    doc = corpus[docnum]\n",
        "    # Get the n-gram as a list of words\n",
        "    ngram_list = str_to_list(ngram_string)\n",
        "    # For each index of a token in the document, up to the last possible n-gram starter\n",
        "    for i in range(len(doc) - len(ngram_list) + 1):\n",
        "        # If the current token is the first word of the n-gram\n",
        "        if doc[i] == ngram_list[0] :\n",
        "                # Init counter of words to 1\n",
        "                c = 1\n",
        "                # While document is not over and still checking for the n-gram\n",
        "                while ( c+i < len(doc) ) and (c < len(ngram_list) ) :\n",
        "                    # If it was not found, break\n",
        "                    if doc[c+i] != ngram_list[c]:\n",
        "                        break\n",
        "                    # Otherwise, increment the counter and go on\n",
        "                    else:\n",
        "                        c += 1\n",
        "                # If all the n words were found, append i as an index for the n_gram\n",
        "                if c == len(ngram_list): \n",
        "                    indices.append(i)\n",
        "\n",
        "    # Then return the list of indices\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pick a max value for n. Remember that if you care about the 7-grams you need the 8-grams to be computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary with the above function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "409mbD-jgrzf",
        "outputId": "ddba2b7f-67d6-4bbd-b564-1553d9de5bf7"
      },
      "outputs": [],
      "source": [
        "dizzo = create_list_of_dict_global(max_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See some frequent 7-grams. Remember that the 7-grams will be in the dictionary in position [6]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_OCFOLIPgdk",
        "outputId": "fad35992-621f-4fa5-f3ae-69618da7bbf1"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "num_to_print = 5\n",
        "min_freq = 10\n",
        "\n",
        "# Print some key-value couples in dizzo[8] with some relevant absolute frequency\n",
        "c = 0\n",
        "for key, value in dizzo[max_n-1].items():\n",
        "    if value[0] >= min_freq:\n",
        "        c += 1\n",
        "        print(key, value)\n",
        "    if c == num_to_print:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **EXPLORING GLUES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a function which computes a given glue, plus all the _tfidf_ values and the _probabilities_ for each document, for each n-gram in the dictionaries. Doing both let us recycle the same iterations to compute both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a list of dictionaries and a required glue\n",
        "# Returns a list with the glue value for each n-gram and their relative tfidf and probs\n",
        "def create_tfidf_and_probs():\n",
        "\n",
        "    # Init two empty lists for list_of_tfdidf_dict and list_of_probs_dict\n",
        "    list_of_tfidf_dicts = []\n",
        "    list_of_probs_dicts = []\n",
        "\n",
        "    # For each n in [1, len(d))\n",
        "    for n in range(1, len(dizzo)):\n",
        "\n",
        "        # Init the two dictionaries\n",
        "        temp_tfidf_dict = dict(dizzo[n - 1])\n",
        "        temp_probs_dict = dict(dizzo[n - 1])\n",
        "\n",
        "        # For each n-gram with its frequencies list\n",
        "        for key, value in dizzo[n - 1].items():\n",
        "            \n",
        "            # Only if n is at least 2 or if it is a monogram of length 3\n",
        "            # Re-compute\n",
        "            if ( n != 1 ) or ( len(key) >= 3 ) :\n",
        "\n",
        "                # Init empty lists for tfidf and probs\n",
        "                tfidf_mod = []\n",
        "                probs = []\n",
        "\n",
        "                # Get the number of documents in which the n-gram was actually found\n",
        "                num_non_zero_doc = ( len(value) - 1 ) / 2\n",
        "\n",
        "                # And for each document in which it was found\n",
        "                # Compute all the tfidf and probs\n",
        "                for doc_idx in range(1, len(value), 2):\n",
        "\n",
        "                    # Get the lengths of the words in the n-gram\n",
        "                    words_lens=[]\n",
        "                    for w in str_to_list(key):\n",
        "                        words_lens.append(len(w))\n",
        "\n",
        "                    # Store the number of the current document\n",
        "                    # and the relative frequency\n",
        "                    num = value[doc_idx]\n",
        "                    rel_freq = value[doc_idx+1]\n",
        "\n",
        "                    # And append in tfidf list the number of the current document\n",
        "                    tfidf_mod.append(num)\n",
        "                    # Followed by its tdidf\n",
        "                    tfidf_mod.append( np.mean(words_lens) * rel_freq * np.log(len(corpus)/num_non_zero_doc) / len(corpus[num]) ) \n",
        "                    \n",
        "                    # And append in probs list the number of the current document\n",
        "                    probs.append(num)\n",
        "                    # Followed by its prob\n",
        "                    probs.append( rel_freq / len(corpus[num]) )\n",
        "\n",
        "                # Compute the average probability of the n-gram\n",
        "                medprob = sum(probs[1::2]) / len(corpus)\n",
        "\n",
        "                # And subtract it from each probability\n",
        "                probs[1::2] = [l1 - medprob for l1 in probs[1::2]]\n",
        "\n",
        "                # Initialize an empty list for the current n-gram twice\n",
        "                temp_tfidf_dict[key] = []\n",
        "                temp_probs_dict[key] = []\n",
        "\n",
        "                # Append the values docnum and tfidf to the list for the current n-gram in the list in the dict\n",
        "                for tfidf in tfidf_mod:\n",
        "                    temp_tfidf_dict[key].append(tfidf)\n",
        "                for p in probs:\n",
        "                    temp_probs_dict[key].append(p)\n",
        "            \n",
        "            # Otherwise if we have a monogram which is not long enough (3 letters at least)\n",
        "            else:\n",
        "                # Remove the list related to such n-gram from both the two dictionaries\n",
        "                temp_tfidf_dict.pop(key)\n",
        "                temp_probs_dict.pop(key)\n",
        "\n",
        "        # Append the two n-th dictionaries to the corresponding list of dictionaries\n",
        "        list_of_tfidf_dicts.append(temp_tfidf_dict)\n",
        "        list_of_probs_dicts.append(temp_probs_dict)\n",
        "\n",
        "    # Return the two lists of dictionaries (the glues for each n-gram are in the first position of their own list, in both)\n",
        "    return list_of_tfidf_dicts, list_of_probs_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And also write a function which only computes the glues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_glues(gluename):\n",
        "    # Initialize an empty list for storing the glue dictionaries\n",
        "    glue_dicts = []\n",
        "\n",
        "    # For each n in [1, len(d))\n",
        "    for n in range(1, len(dizzo)):\n",
        "\n",
        "        # Initialize the glue dictionary as a dictionary with the same keys as dizzo[n-1] associating 0 to each\n",
        "        glue_dict = dict.fromkeys(dizzo[n - 1], 0)\n",
        "\n",
        "        # For each n-gram with its frequencies list\n",
        "        for key, value in dizzo[n - 1].items():\n",
        "            # Only if n is at least 2 or if it is a monogram of length 3\n",
        "            if ( n != 1 ) or ( len(key) >= 3 ):\n",
        "                # Store the absolute frequency for the n-gram\n",
        "                abs_freq = value[0]\n",
        "\n",
        "                # If n is at least 2\n",
        "                if n != 1:\n",
        "                    # Convert the n-gram into a list instead of a string\n",
        "                    key_list = str_to_list(key)\n",
        "\n",
        "                    # Initialize the sum to zero\n",
        "                    s = 0\n",
        "                    \n",
        "                    # Compute the right GLUE coefficient\n",
        "\n",
        "                    # Do Dice\n",
        "                    if gluename == 'Dice':\n",
        "\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = dizzo[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = dizzo[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 + f2) / (n - 1)\n",
        "\n",
        "                        gl = (2 * abs_freq) / s\n",
        "\n",
        "                    # Do SCP\n",
        "                    elif gluename == 'SCP':\n",
        "\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = dizzo[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = dizzo[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 * f2) / (n - 1)\n",
        "\n",
        "                        gl = (abs_freq**2) / s\n",
        "\n",
        "\n",
        "                    elif gluename == 'MI':\n",
        "                        \n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = dizzo[i][list_to_str(key_list[:i+1])][0]\n",
        "                            f2 = dizzo[n-i-2][list_to_str(key_list[i+1:])][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 * f2) / (n - 1)\n",
        "\n",
        "                        gl = np.log(abs_freq / s)\n",
        "                    else:\n",
        "                        gl = 0\n",
        "\n",
        "                    # Add the glue to the list of the current n-gram\n",
        "                    glue_dict[key] = gl\n",
        "\n",
        "            # Otherwise if we have a monogram which is not long enough (3 letters at least)\n",
        "            else:\n",
        "                # Remove the list related to such n-gram from the glue dictionary\n",
        "                glue_dict.pop(key)\n",
        "\n",
        "        # Append the n-th glue dictionary to the list of glue dictionaries\n",
        "        glue_dicts.append(glue_dict)\n",
        "\n",
        "    # Return the list of glue dictionaries\n",
        "    return glue_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the glues for each n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SCP_glues = compute_glues('SCP')\n",
        "dice_glues = compute_glues('Dice')\n",
        "MI_glues = compute_glues('MI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute _tfidfs_ and _probs_ for each document in which they appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidfs, probs = create_tfidf_and_probs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the glues for the 3-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 10 items\n",
        "for key in list(SCP_glues[2].keys())[:10]:\n",
        "    print(f\"SCP: {key} [{SCP_glues[2][key]:.4f}]\")\n",
        "    print(f\"Dice: {key} [{dice_glues[2][key]:.4f}]\")\n",
        "    print(f\"MI: {key} [{MI_glues[2][key]:.4f}]\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the tfidfs dictionary for the 3-grams. Remember, the even indices are document numbers and the odd ones are the _tfidf_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"### TFIDF ###\")\n",
        "\n",
        "# Print the first 10 items\n",
        "for key in list(tfidfs[2].keys())[:10]:\n",
        "    # Empty temp list\n",
        "    formatted_elements = []\n",
        "    # Format numbers and append them to the temp list\n",
        "    for i, num in enumerate(tfidfs[2][key]):\n",
        "        if i % 2 == 0:\n",
        "            # Even index (integer), print normally\n",
        "            formatted_elements.append(str(num))\n",
        "        else:\n",
        "            # Odd index (double), print with four digits after the decimal point\n",
        "            formatted_elements.append(f\"{num:.4f}\")\n",
        "    # Print the key and the formatted string\n",
        "    print(f\"{key}: {formatted_elements}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the probs dictionary for the 3-grams. Remember, the even indices are document numbers and the odd ones are the _prob_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"### PROBS ###\")\n",
        "\n",
        "# Print the first 10 items\n",
        "for key in list(probs[2].keys())[:10]:\n",
        "    # Empty temp list\n",
        "    formatted_elements = []\n",
        "    # Format numbers and append them to the temp list\n",
        "    for i, num in enumerate(probs[2][key]):\n",
        "        if i % 2 == 0:\n",
        "            # Even index (integer), print normally\n",
        "            formatted_elements.append(str(num))\n",
        "        else:\n",
        "            # Odd index (double), print with four digits after the decimal point\n",
        "            formatted_elements.append(f\"{num:.4f}\")\n",
        "    # Print the key and the formatted string\n",
        "    print(f\"{key}: {formatted_elements}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **REGULAR EXPRESSIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute a list with n dictionaries, each associating to each n-gram all the (n+1)-grams which have one more word on the left/right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of dictionaries, one for each value of n\n",
        "fathers = []\n",
        "\n",
        "# Notice n < max_n obviously\n",
        "for n in range(1, max_n):\n",
        "\n",
        "    # Get a dictionary with all the keys of the n-th dictionary in dizzo, and empty lists as values\n",
        "    f = {key: [] for key in dict(dizzo[n - 1]).keys()}\n",
        "\n",
        "    # For each (n+1)-gram in the (n+1)-th dictionary\n",
        "    for key, value in dizzo[n].items():\n",
        "        # Get the (n+1)-gram as list of words\n",
        "        key_list = str_to_list(key)\n",
        "        # Get the two n-grams\n",
        "        subkey1 = list_to_str(key_list[1:])\n",
        "        subkey2 = list_to_str(key_list[:-1])\n",
        "        # Add them in the temp dictionary\n",
        "        f[subkey1].append(key)\n",
        "        f[subkey2].append(key)\n",
        "        \n",
        "    # And finally append the dictionary to the list fathers\n",
        "    fathers.append(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize first ten items for n = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 10 items\n",
        "for key in list(fathers[0].keys())[:10]:\n",
        "    # For each key print the list\n",
        "    print(f\"{key}: {fathers[0][key]}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define the function which returns a dictionary containing the _tfidfs_ and _probs_ lists only for the REs (regardless of n)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auxiliary function (for readability) which checks whether the \"key_string\" n-gram is a RE\n",
        "def process_keys_2(key_string, glues, tdfids, probs, REglues, REtfidfs, REprobs):\n",
        "    # Get n as the number of spaces in key_string + 1\n",
        "    n = key_string.count(' ')\n",
        "    \n",
        "    # Get the glue for the n-gram\n",
        "    glue = glues[n][key_string]\n",
        "\n",
        "    # Get key_string as list to easily remove the first/last word\n",
        "    key_list = str_to_list(key_string)\n",
        "\n",
        "    # Get the set of glues for (n-1)-grams\n",
        "    omega_n_minus = set()\n",
        "    # If it's a 2-gram do not check for the glues of 1-grams\n",
        "    if n > 1:\n",
        "        omega_n_minus.add(glues[n - 1][list_to_str(key_list[1:])])\n",
        "        omega_n_minus.add(glues[n - 1][list_to_str(key_list[:-1])])\n",
        "\n",
        "    # Get the set of glues for (n+1)-grams\n",
        "    omega_n_plus = set([glues[n + 1][fath] for fath in fathers[n][key_string]])\n",
        "    # If the glue is bigger than the glue for all the sons and fathers\n",
        "    if all( ( glue > g ) for g in omega_n_minus.union(omega_n_plus) ):\n",
        "        # Add the info to the two dictionaries\n",
        "        REglues[key_string] = glues[n][key_string]\n",
        "        REtfidfs[key_string] = tdfids[n][key_string]\n",
        "        REprobs[key_string] = probs[n][key_string]\n",
        "\n",
        "# Returns two dictionaries, only containing REs as keys\n",
        "def find_RE(tfidfs, probs, glues):\n",
        "    # Init the new dictionaries containing only the REs\n",
        "    REtfidfs = {}\n",
        "    REprobs = {}\n",
        "    REglues = {}\n",
        "\n",
        "    # For each n in [1, max_n)\n",
        "    for n in range(1, len(glues)-1):\n",
        "        # For each n-gram with their list of tfidfs\n",
        "        for key, value in glues[n].items() :\n",
        "            # Process the n-gram to decide whether it is a RE\n",
        "            process_keys_2(key, glues, tfidfs, probs, REglues, REtfidfs, REprobs)\n",
        "\n",
        "    # Return the two dictionaries\n",
        "    return REtfidfs, REprobs, REglues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then compute the REsn information using the three glues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RE_SCP_tfidfs, RE_SCP_probs, RE_SCP_glues = find_RE(tfidfs, probs, SCP_glues)\n",
        "RE_dice_tfidfs, RE_dice_probs, RE_dice_glues = find_RE(tfidfs, probs, dice_glues)\n",
        "RE_MI_tfidfs, RE_MI_probs, RE_MI_glues = find_RE(tfidfs, probs, MI_glues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And visualize the REs with their glues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 10 items\n",
        "\n",
        "for key in list(RE_SCP_glues.keys())[:10]:\n",
        "    print(f\"SCP: {key} [{RE_SCP_glues[key]:.4f}]\")\n",
        "print()\n",
        "\n",
        "for key in list(RE_dice_glues.keys())[:10]:\n",
        "    print(f\"Dice: {key} [{RE_dice_glues[key]:.4f}]\")\n",
        "print() \n",
        "\n",
        "for key in list(RE_MI_glues.keys())[:10]:    \n",
        "    print(f\"MI: {key} [{RE_MI_glues[key]:.4f}]\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **FILTERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to delete REs containing special characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gets a string and returns false if must be deleted\n",
        "def no_special(key_string):\n",
        "    for i in range(len(key_string)):\n",
        "        if (key_string[i] in specialchars):\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to delete REs contained in one only document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gets a string and returns false if must be deleted\n",
        "def more_documents(key_list):\n",
        "    if (dizzo[len(str_to_list(key_list)) - 1][key_list][0] > 1):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remember REs datastructures are now just dictionaries, not lists of dictionaries\n",
        "RE_SCP_glues_filtered = {}\n",
        "RE_SCP_tfidfs_filtered = {}\n",
        "RE_SCP_probs_filtered = {}\n",
        "\n",
        "RE_dice_glues_filtered = {}\n",
        "RE_dice_tfidfs_filtered = {}\n",
        "RE_dice_probs_filtered = {}\n",
        "\n",
        "RE_MI_glues_filtered = {}\n",
        "RE_MI_tfidfs_filtered = {}\n",
        "RE_MI_probs_filtered = {}\n",
        "\n",
        "# Iterate through REs and filter SCP REs\n",
        "for key, value in RE_SCP_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_SCP_glues_filtered[key] = value\n",
        "        RE_SCP_tfidfs_filtered[key] = RE_SCP_tfidfs[key]\n",
        "        RE_SCP_probs_filtered[key] = RE_SCP_probs[key]\n",
        "\n",
        "# Iterate through REs and filter Dice REs\n",
        "for key, value in RE_dice_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_dice_glues_filtered[key] = value\n",
        "        RE_dice_tfidfs_filtered[key] = RE_dice_tfidfs[key]\n",
        "        RE_dice_probs_filtered[key] = RE_dice_probs[key]\n",
        "\n",
        "# Iterate through REs and filter MI REs\n",
        "for key, value in RE_MI_glues.items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        RE_MI_glues_filtered[key] = value\n",
        "        RE_MI_tfidfs_filtered[key] = RE_MI_tfidfs[key]\n",
        "        RE_MI_probs_filtered[key] = RE_MI_probs[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the glues for some filtered REs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print 10 items\n",
        "\n",
        "# Find the intersection of keys\n",
        "intersection_keys = set(RE_SCP_glues_filtered.keys()).intersection(set(RE_dice_glues_filtered.keys()), set(RE_MI_glues_filtered.keys()))\n",
        "\n",
        "# Print\n",
        "for key in list(intersection_keys)[:10]:\n",
        "    print(f\"SCP: {key} [{RE_SCP_glues_filtered[key]:.4f}]\")\n",
        "    print(f\"Dice: {key} [{RE_dice_glues_filtered[key]:.4f}]\")\n",
        "    print(f\"MI: {key} [{RE_MI_glues_filtered[key]:.4f}]\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **STOP WORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to find the stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns the extracted stop words\n",
        "def get_stop_words():\n",
        "\n",
        "    # Initialize a dictionary for ints with 0 as default value\n",
        "    neighbour_counts = defaultdict(int)\n",
        "    # Count how many neightbours each word has across the corpus\n",
        "    for doc in corpus:\n",
        "        # Increment by 1 the count for the first and last word of the current doc\n",
        "        neighbour_counts[doc[0]] += 1\n",
        "        neighbour_counts[doc[-1]] += 1\n",
        "        # From second to second last word increment by 2\n",
        "        for idx in range(1, len(doc) - 1):\n",
        "            neighbour_counts[doc[idx]] += 2\n",
        "    # And get the keys sorted by value\n",
        "    neighbour_counts = sorted(neighbour_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "    \n",
        "    # Now look for the elbow point in the list\n",
        "    elbow_point_index = 0\n",
        "    max_tangens = 0\n",
        "    # Iterate over the list (besides last word) to find the elbow point\n",
        "    for index in range(len(neighbour_counts) - 1):\n",
        "        # Get the difference between the count for the current word and the next\n",
        "        neighbour_diff = neighbour_counts[index][1] - neighbour_counts[index + 1][1]\n",
        "        # Apply the rule (find the tangents difference)\n",
        "        tangents_diff = abs(math.tan(neighbour_counts[index][1] + neighbour_diff) - math.tan(neighbour_counts[index][1]))\n",
        "        # Update the elbow point if the current difference is greater than the maximum difference\n",
        "        if tangents_diff > max_tangens:\n",
        "            elbow_point_index = index\n",
        "            max_tangens = tangents_diff\n",
        "    \n",
        "    # Get the couples corresponding to all the words up to the elbow (highest num of neighbours)\n",
        "    stop_word_counts = neighbour_counts[:elbow_point_index+1]\n",
        "    # Get the stop words\n",
        "    stop_words = [tuple[0] for tuple in stop_word_counts]\n",
        "    \n",
        "    # Return the stop words and the filtered expressions\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get stop words\n",
        "stop_words = get_stop_words()\n",
        "\n",
        "# And write them all on file\n",
        "with open('../Output/stop_words.txt', 'w') as f:\n",
        "    for word in stop_words:\n",
        "        f.write(\"\\\"\")\n",
        "        f.write(word)\n",
        "        f.write(\"\\\"\")\n",
        "        f.write(', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now filter out the expressions containing stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All the REs in the corresponding list of n-grams if the first and the last word are not stop-words\n",
        "RE_SCP_glues_filtered = [re for re in RE_SCP_glues_filtered.keys() if re[0] not in stop_words and re[-1] not in stop_words]\n",
        "RE_dice_glues_filtered = [re for re in RE_dice_glues_filtered.keys() if re[0] not in stop_words and re[-1] not in stop_words]\n",
        "RE_MI_glues_filtered = [re for re in RE_MI_glues_filtered.keys() if re[0] not in stop_words and re[-1] not in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many REs did we get overall?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print\n",
        "print(str(len(RE_SCP_glues_filtered)) + ' with SCP')\n",
        "print(str(len(RE_dice_glues_filtered)) + ' with Dice')\n",
        "print(str(len(RE_MI_glues_filtered)) + ' with MI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now print 200 random REs on file, for each glue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 200 random REs for each\n",
        "REs_200_SCP = np.array(RE_SCP_glues_filtered)[np.random.choice( len(RE_SCP_glues_filtered), 200 )]\n",
        "REs_200_dice = np.array(RE_dice_glues_filtered)[np.random.choice( len(RE_dice_glues_filtered), 200 )]\n",
        "REs_200_MI = np.array(RE_MI_glues_filtered)[np.random.choice( len(RE_MI_glues_filtered), 200 )]\n",
        "\n",
        "# And write them all on file\n",
        "with open('../Output/200_random_REs.txt', 'w') as f:\n",
        "\n",
        "    f.write('SCP\\n')\n",
        "    for line in REs_200_SCP:\n",
        "        f.write(line + '\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    f.write('Dice\\n')\n",
        "    for line in REs_200_dice:\n",
        "        f.write(line + '\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    f.write('MI\\n')\n",
        "    for line in REs_200_MI:\n",
        "        f.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **EXPLICIT KEYWORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select _tfidfs_ about 1-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init empty dictionary\n",
        "uni_tfidfs_filtered = {}\n",
        "# For each n in [1, max_n)\n",
        "for key, value in tfidfs[0].items():\n",
        "    if no_special(key) and more_documents(key) :\n",
        "        uni_tfidfs_filtered[key] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to find explicit keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returnsn a dictionary which associates to each document its explicit keywords\n",
        "# Also return a dictionary which associates to each document the REs with at least two words\n",
        "def findExplicit_Keywords(REs_tfidfs, uni_tfidfs, uni_max, multi_max):\n",
        "\n",
        "    # Initialize dictionaries for unigrams and n-grams matches\n",
        "    uni_REs_per_doc = {}\n",
        "    multi_REs_per_doc = {}\n",
        "    for k in range(len(corpus)):\n",
        "        uni_REs_per_doc['doc' + str(k)] = []\n",
        "        uni_REs_per_doc['tfidf' + str(k)] = []\n",
        "        multi_REs_per_doc['doc' + str(k)] = []\n",
        "        multi_REs_per_doc['tfidf' + str(k)] = []\n",
        "\n",
        "    # Populate the unigram dictionary\n",
        "    for key, value in uni_tfidfs.items():\n",
        "        for index in range(0, len(value), 2):\n",
        "            doc = value[index]\n",
        "            uni_REs_per_doc['doc' + str(doc)].append(key)\n",
        "            uni_REs_per_doc['tfidf' + str(doc)].append(value[index + 1])\n",
        "    # Sort the unigrams by their tfidf scores and limit the number of unigrams\n",
        "    uni_expks_per_doc = {}\n",
        "    for k in range(len(corpus)):\n",
        "        sorted_uni = sorted(zip(uni_REs_per_doc['tfidf' + str(k)], uni_REs_per_doc['doc' + str(k)]), reverse=True)\n",
        "        uni_expks_per_doc['doc' + str(k)] = [x for _, x in sorted_uni][:uni_max]\n",
        "\n",
        "    # Populate the REs match dictionary\n",
        "    for key, value in REs_tfidfs.items():\n",
        "        for index in range(0, len(value), 2):\n",
        "            doc = value[index]\n",
        "            multi_REs_per_doc['doc' + str(int(doc))].append(key)\n",
        "            multi_REs_per_doc['tfidf' + str(int(doc))].append(value[index + 1])\n",
        "    # Sort the REs matches by their tfidf scores and limit the number of matches\n",
        "    multi_expks_per_doc = {}\n",
        "    for k in range(len(corpus)):\n",
        "        sorted_re = sorted(zip(multi_REs_per_doc['tfidf' + str(k)], multi_REs_per_doc['doc' + str(k)]), reverse=True)\n",
        "        multi_expks_per_doc['doc' + str(k)] = [x for _, x in sorted_re][:multi_max]\n",
        "\n",
        "    # Combine the unigram and REs match dictionaries\n",
        "    expks_per_doc = dict(uni_expks_per_doc)\n",
        "    for key, value in uni_expks_per_doc.items():\n",
        "        expks_per_doc[key].extend(multi_expks_per_doc[key])\n",
        "\n",
        "    return expks_per_doc, multi_REs_per_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute keywords for both 1-grams and not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose number of keywords\n",
        "uni_max = 5\n",
        "multi_max = 10\n",
        "\n",
        "# Get the explicit keywords\n",
        "SCP_explicit_keywords, SCP_REs_per_doc = findExplicit_Keywords(RE_SCP_tfidfs_filtered, uni_tfidfs_filtered, uni_max, multi_max)\n",
        "dice_explicit_keywords, dice_REs_per_doc = findExplicit_Keywords(RE_dice_tfidfs_filtered, uni_tfidfs_filtered, uni_max, multi_max)\n",
        "MI_explicit_keywords, MI_REs_per_doc = findExplicit_Keywords(RE_MI_tfidfs_filtered, uni_tfidfs_filtered, uni_max, multi_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now print them on file, for each glue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write them all on file\n",
        "with open('../Output/explicit_keywords.txt', 'w') as f:\n",
        "\n",
        "    # Write keywords for each document using SCP\n",
        "    f.write('SCP\\n')\n",
        "    for key, value in SCP_explicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    # Write keywords for each document using Dice\n",
        "    f.write('Dice\\n')\n",
        "    for key, value in dice_explicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    # Write keywords for each document using MI\n",
        "    f.write('MI\\n')\n",
        "    for key, value in MI_explicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **IMPLICIT KEYWORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function to get the dictionary with correlation values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score(a, b):\n",
        "    # Initialize indices and sum\n",
        "    i = 0\n",
        "    j = 0\n",
        "    s = 0\n",
        "    # Loop over the elements of a and b\n",
        "    while j < len(b[::2]) and i < len(a[::2]):\n",
        "        while j < len(b[::2]) and a[::2][i] >= b[::2][j]:\n",
        "            # If the elements are equal, add their product to the sum\n",
        "            if a[::2][i] == b[::2][j]:\n",
        "                s += a[2*i+1] * b[2*j+1]\n",
        "            j += 1\n",
        "        i += 1\n",
        "    # Return the score\n",
        "    return 1000000 * s / (len(corpus) - 1)\n",
        "\n",
        "def create_corr_dict(REs_probs):\n",
        "    # Copy the input dictionary and initialize new dictionaries\n",
        "    temp_REs_probs = dict(REs_probs)\n",
        "    temp_corr_scores = {}\n",
        "    correlation_dict = {}\n",
        "\n",
        "    # For each RE\n",
        "    for key1, value1 in REs_probs.items():\n",
        "        key1_connections = {}\n",
        "        # For each RE (cartesian product)\n",
        "        for key2, value2 in temp_REs_probs.items():\n",
        "            # Calculate the score among the two \n",
        "            cv = score(value1, value2)\n",
        "            # If the score is non-zero, add it to the dictionary\n",
        "            if abs(cv) > 0:\n",
        "                key1_connections[key2] = cv\n",
        "        # Add the dictionary to the output dictionary at key1\n",
        "        temp_corr_scores[key1] = key1_connections\n",
        "        # Remove the item from the copied dictionary so that it won't be computed twice\n",
        "        temp_REs_probs.pop(key1)\n",
        "\n",
        "    # For each of the dictionaries which were just computed\n",
        "    for key1, value1 in temp_corr_scores.items():\n",
        "        temp_correlations = {}\n",
        "        # Get the score of key3 with key3 itself\n",
        "        covkey3 = value1[key1]\n",
        "        # For each word in the connections dictionary for key3\n",
        "        for key2, value2 in value1.items():\n",
        "            # Compute the corr\n",
        "            corr = value2 / (np.sqrt(covkey3) * np.sqrt(temp_corr_scores[key2][key2]))\n",
        "            # If the correlation is non-zero, add it to the dictionary\n",
        "            if abs(corr) > 0.0000000001:\n",
        "                temp_correlations[key2] = corr\n",
        "        # Add the dictionary to the final dictionary\n",
        "        correlation_dict[key1] = temp_correlations\n",
        "\n",
        "    return correlation_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then compute them, for each glue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SCP\n",
        "RE_SCP_dict_corr = create_corr_dict(RE_SCP_probs_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dice\n",
        "RE_dice_dict_corr = create_corr_dict(RE_dice_probs_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MI\n",
        "RE_MI_dict_corr = create_corr_dict(RE_MI_probs_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function which computes the implicit keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findImplicit_Keywords(dict_cov_re, Explicit_Keywords, re_in_doc, numberImplicitKeywords, firstexplmultikeyword, numkeyscore):\n",
        "    # Initialize scores dictionary\n",
        "    scores = {}\n",
        "    for k in range(len(corpus)):\n",
        "        scores['doc' + str(k)] = []\n",
        "        scores['scores' + str(k)] = []\n",
        "\n",
        "    # Loop over each document in the corpus\n",
        "    for doc in range(len(corpus)):\n",
        "        # Loop over each document again for comparison\n",
        "        for doc1 in range(len(corpus)):\n",
        "            # Skip if the documents are the same\n",
        "            if doc != doc1:\n",
        "                # Loop over each keyword in the second document\n",
        "                for re in re_in_doc['doc' + str(doc1)]:\n",
        "                    # Skip if the keyword is already in the scores or in the first document\n",
        "                    if re not in scores['doc' + str(doc)] and re not in re_in_doc['doc' + str(doc)]:\n",
        "                        somma = 0\n",
        "                        # Loop over each explicit keyword in the first document\n",
        "                        for j in range(min(numkeyscore, len(Explicit_Keywords[\"doc\" + str(doc)][firstexplmultikeyword:]))):\n",
        "                            keyword = Explicit_Keywords[\"doc\" + str(doc)][j + firstexplmultikeyword]\n",
        "                            # Add the covariance score if the keyword is in the covariance dictionary\n",
        "                            if keyword in dict_cov_re[re]:\n",
        "                                somma += dict_cov_re[re][keyword]\n",
        "                            # Add the covariance score if the keyword is in the covariance dictionary\n",
        "                            elif re in dict_cov_re[keyword]:\n",
        "                                somma += dict_cov_re[keyword][re]\n",
        "                        # Append the keyword and its score to the scores dictionary\n",
        "                        scores['doc' + str(doc)].append(re)\n",
        "                        scores['scores' + str(doc)].append(somma / numkeyscore)\n",
        "\n",
        "    # Initialize the final dictionary for implicit keywords\n",
        "    dict_implkey_re_final = {}\n",
        "    for k in range(len(corpus)):\n",
        "        # Sort the keywords by their scores and add them to the final dictionary\n",
        "        dict_implkey_re_final['doc' + str(k)] = [x for _, x in sorted(zip(scores['scores' + str(k)], scores['doc' + str(k)]), reverse=True)]\n",
        "        # Limit the number of keywords to the specified number\n",
        "        if len(dict_implkey_re_final['doc' + str(k)]) >= numberImplicitKeywords:\n",
        "            dict_implkey_re_final['doc' + str(k)] = dict_implkey_re_final['doc' + str(k)][:numberImplicitKeywords]\n",
        "\n",
        "    return dict_implkey_re_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then compute everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "max = 5\n",
        "first = 5\n",
        "many = 10\n",
        "\n",
        "# Compute\n",
        "SCP_implicit_keywords = findImplicit_Keywords(RE_SCP_dict_corr, SCP_explicit_keywords, SCP_REs_per_doc, max, first, many)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute\n",
        "# dice_implicit_keywords = findImplicit_Keywords(RE_dice_dict_corr, dice_explicit_keywords, dice_REs_per_doc, max, first, many)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute\n",
        "# MI_implicit_keywords = findImplicit_Keywords(RE_MI_dict_corr, MI_explicit_keywords, MI_REs_per_doc, max, first, many)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally print on file, for each glue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print\n",
        "with open('../Output/implicit_keywords.txt', 'w') as f:\n",
        "\n",
        "    # Print using SCP\n",
        "    f.write('SCP\\n')\n",
        "    for key,value in SCP_implicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    '''\n",
        "    # Print using Dice\n",
        "    f.write('Dice\\n')\n",
        "    for key,value in dice_implicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "    # Print using MI\n",
        "    f.write('MI\\n')\n",
        "    for key,value in MI_implicit_keywords.items():\n",
        "        f.write(key)\n",
        "        f.write(': ')\n",
        "        for v in value:\n",
        "            f.write(v)\n",
        "            f.write('; ')\n",
        "        f.write('\\n')\n",
        "    f.write('\\n')\n",
        "    '''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
