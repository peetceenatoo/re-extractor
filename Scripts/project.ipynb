{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GuTl2RIXgsQA"
      },
      "outputs": [],
      "source": [
        "# Library for reading and writing data to and from files\n",
        "import os\n",
        "# Library for numerical computing\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Z5sOc92TgsLE"
      },
      "outputs": [],
      "source": [
        "# Dataset directory\n",
        "corpus_directory='../Dataset/corpus2mw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To iterate the reading procedure, get the names of the documents in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3170"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a list with the names of the documents\n",
        "texts_names = os.listdir(corpus_directory)\n",
        "\n",
        "len(texts_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Get tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, define the special characters to be separated from each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "HZYLcWMUgsNo"
      },
      "outputs": [],
      "source": [
        "# List of characters\n",
        "specialchars = [';', ':', '!', '?', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-']\n",
        "# specialchars = [';', ':', '!', '?', '<', '>', '&', ')', '(', ']', '[', ',', '.', '\"', '%', '$ ', '=', '}', '{', '-']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And define the function which separates the special characters from each word, assuming they can only be before or after each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mvTiLGhygsIh"
      },
      "outputs": [],
      "source": [
        "def token(w):\n",
        "    # Init the empty list of tokens\n",
        "    res = []\n",
        "\n",
        "    # If the length is 1, add the character whatever it is\n",
        "    if len(w) == 1:\n",
        "        res.append(w)\n",
        "    \n",
        "    # Otherwise (if it's at least two characters)\n",
        "    else:\n",
        "        # If the first character is special, add it to the list and remove it from the word\n",
        "        if w[0] in specialchars:\n",
        "            res.append(w[0])\n",
        "            w = w[1:]\n",
        "        \n",
        "        # Now, if the length became 1 because of that, for the same reason as before, add the character whatever it is\n",
        "        if len(w) == 1:\n",
        "            res.append(w)\n",
        "        # Otherwise (if it's at least two characters), both if I had removed the first or not\n",
        "        # Check whether the last character is special\n",
        "        elif w[-1] in specialchars:\n",
        "            res.append(w[:-1])\n",
        "            res.append(w[-1])\n",
        "        # or not\n",
        "        else:\n",
        "            res.append(w)\n",
        "        \n",
        "    # Return the list of tokens\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each document, for each word (in each line), if either the last or the first character are specialchars, then split in multiple tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "# For each document in the directory\n",
        "for text in texts_names:\n",
        "\n",
        "    # Init a temp empty list for the words in the current document\n",
        "    words = []\n",
        "\n",
        "    # Open the document\n",
        "    with open(corpus_directory + '/' + text, 'r', errors='ignore') as file:\n",
        "\n",
        "        # For each line\n",
        "        for line in file:\n",
        "            # For each word in the line\n",
        "            for word in line.split():\n",
        "                # Tokenize it\n",
        "                aux = token(word)\n",
        "                # And add each token to the list of words\n",
        "                for t in aux:\n",
        "                    words.append(t)\n",
        "    # Then append the list of tokens for the document in the corpus list\n",
        "    corpus.append(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way, the corpus is a list of documents, which are lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DfPdppoOYO7",
        "outputId": "948487ce-c872-4b4c-fce7-97fd93d5f5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Koalas', 'have', 'few', 'predators', ';', 'dingos', 'and', 'large', 'pythons', 'may']\n"
          ]
        }
      ],
      "source": [
        "# Visualize first 10 tokens in the second document corpus[1]\n",
        "print(corpus[1][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to get a list of words from a string with words separated with a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "l88ONep-gr5i"
      },
      "outputs": [],
      "source": [
        "# Given a list of strings, it returns a string\n",
        "# in which the substrings will be separated by ' '\n",
        "def list_to_str(strings):\n",
        "    # Init res as an empty string\n",
        "    res = \"\"\n",
        "    # For each character/string in the list\n",
        "    for i in range(len(strings)):\n",
        "        # Concatenate the string plus a space to res\n",
        "        res += strings[i] + ' '\n",
        "    # Then return everything besides the last space\n",
        "    return res[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to do the opposite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a string which contains substrings separated by ' ',\n",
        "# it returns a list of words\n",
        "def str_to_list(s):\n",
        "    # Init res as an empty list\n",
        "    res = []\n",
        "    # Split the string by ' ' and for each substring\n",
        "    for word in s.split():\n",
        "        # Append the substring to res\n",
        "        res.append(word)\n",
        "    # Return the list of substrings\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Declare a function which returns a list of n dictionaries, one for each n up to a fixed max, which contain the information about the frequencies for each n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n dictionaries, one for all the possible n-grams, with n in [1, max]\n",
        "# Each n-dictionary will map each n-gram to a list with the absolute frequency in [0]\n",
        "# and then the index of the documents in which it was found once at least,\n",
        "# followed by the relative frequency (e.g. [25, 1, 20, 2, 1, 3, 4])\n",
        "def create_list_of_dict_global(max):\n",
        "    # Init an empty list of dictionaries\n",
        "    list_dict=[]\n",
        "\n",
        "    # For each n in [1, max], append an empty dictionary to the list\n",
        "    for n in range(max):\n",
        "        list_dict.append({})\n",
        "\n",
        "    # For each index of a document in the corpus\n",
        "    for i in range(len(corpus)):\n",
        "        # For each index of a token in the document\n",
        "        for t in range(len(corpus[i])):\n",
        "            # For each n in [1, max]\n",
        "            for n in range(1, len(list_dict)+1):\n",
        "                # If the document is not over (there is still space for an n-gram)\n",
        "                if ( t + n ) <= len(corpus[i]) :\n",
        "                    # If the n-gram is not yet in the n-dictionary\n",
        "                    if not ( list_to_str(corpus[i][t : t+n]) in list_dict[n-1].keys() ) :\n",
        "                        # Associate to the new n-gram an empty list\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])] = []\n",
        "                        # Set the frequency to 1 in position [0]\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                        # And then append the index of the current document\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                        # and set the relative frequency to 1\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "                    else:\n",
        "                        # Add one to the frequency of the n-gram\n",
        "                        list_dict[n-1][list_to_str(corpus[i][t : t+n])][0] += 1\n",
        "                        # And if the last document in which this n-gram was found\n",
        "                        # is the current one\n",
        "                        if list_dict[n-1][list_to_str(corpus[i][t : t+n])][-2] == i :\n",
        "                            # Just increment the relative frequency\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])][-1] += 1\n",
        "                        # Otherwise\n",
        "                        else:\n",
        "                            # Append the new (current) document\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(i)\n",
        "                            # and set the relative frequency to 1\n",
        "                            list_dict[n-1][list_to_str(corpus[i][t : t+n])].append(1)\n",
        "    # Then return the list of dictionaries\n",
        "    return list_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to find all the indeces at which a given n-gram occurs in a given document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n dictionaries, one for all the possible n-grams, with n in [1, max]\n",
        "def find_indices_ngram_doc(ngram_string, docnum):\n",
        "    # Init an empty list for the indices\n",
        "    indices = []\n",
        "    # Get the document as a list of tokens \n",
        "    doc = corpus[docnum]\n",
        "    # Get the n-gram as a list of words\n",
        "    ngram_list = str_to_list(ngram_string)\n",
        "    # For each index of a token in the document, up to the last possible n-gram starter\n",
        "    for i in range(len(doc) - len(ngram_list) + 1):\n",
        "        # If the current token is the first word of the n-gram\n",
        "        if doc[i] == ngram_list[0] :\n",
        "                # Init counter of words to 1\n",
        "                c = 1\n",
        "                # While document is not over and still checking for the n-gram\n",
        "                while ( c+i < len(doc) ) and (c < len(ngram_list) ) :\n",
        "                    # If it was not found, break\n",
        "                    if doc[c+i] != ngram_list[c]:\n",
        "                        break\n",
        "                    # Otherwise, increment the counter and go on\n",
        "                    else:\n",
        "                        c += 1\n",
        "                # If all the n words were found, append i as an index for the n_gram\n",
        "                if c == len(ngram_list): \n",
        "                    indices.append(i)\n",
        "\n",
        "    # Then return the list of indices\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pick a max value for n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary with the above function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "409mbD-jgrzf",
        "outputId": "ddba2b7f-67d6-4bbd-b564-1553d9de5bf7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dizzo \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_list_of_dict_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_n\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[77], line 36\u001b[0m, in \u001b[0;36mcreate_list_of_dict_global\u001b[1;34m(max)\u001b[0m\n\u001b[0;32m     33\u001b[0m list_dict[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][list_to_str(corpus[i][t : t\u001b[38;5;241m+\u001b[39mn])][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# And if the last document in which this n-gram was found\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# is the current one\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m list_dict[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[43mlist_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m i :\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Just increment the relative frequency\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     list_dict[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][list_to_str(corpus[i][t : t\u001b[38;5;241m+\u001b[39mn])][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Otherwise\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Append the new (current) document\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[75], line 7\u001b[0m, in \u001b[0;36mlist_to_str\u001b[1;34m(strings)\u001b[0m\n\u001b[0;32m      5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# For each character/string in the list\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Concatenate the string plus a space to res\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m strings[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Then return everything besides the last space\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dizzo = create_list_of_dict_global(max_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that the 7-grams will be in the dictionary in position [6]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greek Christian scribes played a crucial role [1, 0, 1]\n",
            "Christian scribes played a crucial role in [1, 0, 1]\n",
            "scribes played a crucial role in the [1, 0, 1]\n",
            "played a crucial role in the preservation [1, 0, 1]\n",
            "a crucial role in the preservation of [1, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "# Set parameters\n",
        "num_to_print = 5\n",
        "\n",
        "# Print\n",
        "for i in list(dizzo[6].keys())[:5]:\n",
        "    print(i, dizzo[6][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_OCFOLIPgdk",
        "outputId": "fad35992-621f-4fa5-f3ae-69618da7bbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ". The median income for a household [30, 2, 1, 369, 3, 2216, 1, 2326, 1, 2803, 4, 2832, 1, 2846, 1, 2857, 1, 2874, 1, 2885, 1, 2896, 3, 2920, 1, 2953, 1, 2962, 1, 2979, 2, 3025, 2, 3049, 2, 3074, 1, 3081, 1, 3111, 1]\n",
            "The median income for a household in [34, 2, 1, 369, 3, 2216, 1, 2326, 1, 2803, 4, 2832, 1, 2846, 1, 2857, 2, 2874, 1, 2885, 1, 2896, 3, 2920, 1, 2936, 1, 2953, 1, 2962, 1, 2979, 2, 3025, 3, 3049, 3, 3074, 1, 3081, 1, 3111, 1]\n",
            "median income for a household in the [36, 2, 1, 369, 3, 2216, 2, 2326, 1, 2803, 4, 2832, 1, 2846, 1, 2857, 2, 2874, 1, 2885, 1, 2896, 3, 2920, 1, 2936, 1, 2953, 1, 2962, 1, 2979, 2, 2986, 1, 3025, 3, 3049, 3, 3074, 1, 3081, 1, 3111, 1]\n",
            ", and the median income for a [35, 2, 1, 369, 3, 2216, 1, 2326, 1, 2803, 4, 2832, 1, 2846, 1, 2857, 2, 2874, 1, 2885, 1, 2896, 3, 2920, 1, 2936, 1, 2953, 1, 2962, 1, 2979, 2, 2986, 1, 3025, 3, 3049, 3, 3074, 1, 3081, 1, 3111, 1]\n",
            "and the median income for a family [36, 2, 1, 369, 3, 2216, 2, 2326, 1, 2803, 4, 2832, 1, 2846, 1, 2857, 2, 2874, 1, 2885, 1, 2896, 3, 2920, 1, 2936, 1, 2953, 1, 2962, 1, 2979, 2, 2986, 1, 3025, 3, 3049, 3, 3074, 1, 3081, 1, 3111, 1]\n"
          ]
        }
      ],
      "source": [
        "# Set parameters\n",
        "num_to_print = 5\n",
        "min_freq = 10\n",
        "\n",
        "# Print some key-value couples in dizzo[8] with some relevant absolute frequency\n",
        "c = 0\n",
        "for key, value in dizzo[max_n-1].items():\n",
        "    if value[0] >= min_freq:\n",
        "        c += 1\n",
        "        print(key, value)\n",
        "    if c == num_to_print:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **EXPLORING GLUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWm0pRcsi5HA"
      },
      "outputs": [],
      "source": [
        "# Given a list of dictionaries and a required glues\n",
        "def create_glue_and_tfidfmod_and_probs(d, gluename):\n",
        "\n",
        "    # Init two empty lists for list_of_tfdidf_dict and list_of_probs_dict\n",
        "    list_of_tfidf_dicts = []\n",
        "    list_of_probs_dicts = []\n",
        "    \n",
        "    # If Dice is the required glue\n",
        "    if gluename == 'Dice':\n",
        "\n",
        "        # For each n in [1, len(d)]\n",
        "        for n in range(1, len(d)+1):\n",
        "\n",
        "            # Get the dictionary of n-grams twice\n",
        "            # ( Consider replacing with {} )\n",
        "            temp_tfidf_dict = dict(d[n - 1])\n",
        "            temp_probs_dict = dict(d[n - 1])\n",
        "\n",
        "            # For each couple n-gram / list of frequencies\n",
        "            for key, value in dict(d[n - 1]).items():\n",
        "\n",
        "                # Only if n is at least 2 or if it is a monogram of length 3\n",
        "                # Re-compute\n",
        "                if ( n != 1 ) or ( len(key) >= 3 ) :\n",
        "\n",
        "                    # Init empty lists for tfidf and probs\n",
        "                    tfidf_mod = []\n",
        "                    probs = []\n",
        "\n",
        "                    # Get the number of documents in which the n-gram was actually found\n",
        "                    num_non_zero_doc = ( len(value) - 1 ) / 2\n",
        "\n",
        "                    # And for each document in which it was found\n",
        "                    for doc_idx in range(1, len(value), 2):\n",
        "\n",
        "                        # Get the lengths of the words in the n-gram\n",
        "                        words_lens=[]\n",
        "                        for w in str_to_list(key):\n",
        "                            words_lens.append(len(w))\n",
        "\n",
        "                        # Store the number of the current document\n",
        "                        # and the relative frequency\n",
        "                        num = value[doc_idx]\n",
        "                        rel_freq = value[doc_idx+1]\n",
        "                        \n",
        "                        # And append in tfidf list the number of the current document\n",
        "                        tfidf_mod.append(num)\n",
        "                        # Followed by its tdidf\n",
        "                        tfidf_mod.append( np.mean(words_lens) * rel_freq * np.log(len(corpus)/num_non_zero_doc) / len(corpus[num]) )\n",
        "                        \n",
        "                        # And append in probs list the number of the current document\n",
        "                        probs.append(num)\n",
        "                        # Followed by its prob\n",
        "                        probs.append( rel_freq / len(corpus[num]) )\n",
        "\n",
        "                    # Compute the average probability of the n-gram\n",
        "                    medprob = sum(probs[1::2])/len(corpus)\n",
        "                    # And subtract it from each probability\n",
        "                    probs[1::2] = [ ( l1 - medprob ) for l1 in probs[1::2] ]\n",
        "\n",
        "                    # Store the absolute frequency for the n-gram\n",
        "                    abs_freq = value[0]\n",
        "\n",
        "                    # Initialize an empty list for the current n-gram twice\n",
        "                    temp_tfidf_dict[key] = []\n",
        "                    temp_probs_dict[key] = []\n",
        "\n",
        "                    # If n is at least 2\n",
        "                    if n != 1:\n",
        "                        # Convert the n-gram into a list instead of a string\n",
        "                        key_list = str_to_list(key)\n",
        "\n",
        "                        # Initialize the sum to zero\n",
        "                        s = 0\n",
        "                        # Dividing the n-gram into two parts w1...wi and wi+1...wn\n",
        "                        for i in range(len(key_list) - 1):\n",
        "                            # Get the absolute frequencies of the two sub-n-grams\n",
        "                            f1 = d[i][key[:i+1]][0]\n",
        "                            f2 = d[n - i - 2][key[i+1:]][0]\n",
        "                            # And add to the sum the partial sum\n",
        "                            s += (f1 + f2) / (n - 1)\n",
        "\n",
        "                        # Append the Dice coefficient to the list of the current n-gram\n",
        "                        # (first value of the list associated to the current n-gram)\n",
        "                        temp_tfidf_dict[key].append((2 * abs_freq) / s)\n",
        "                        temp_probs_dict[key].append((2 * abs_freq) / s)\n",
        "                    \n",
        "                    # Append the values docnum and tfidf to the list for the current n-gram in the list in the dict\n",
        "                    for tfidf in tfidf_mod:\n",
        "                        temp_tfidf_dict[key].append(tfidf)\n",
        "                    for p in probs:\n",
        "                        temp_probs_dict[key].append(p)\n",
        "\n",
        "                # Otherwise if we have a monogram which is not long enough (3 letters at least)\n",
        "                else:\n",
        "                    # Remove the list related to such n-gram from both the two dictionaries\n",
        "                    temp_tfidf_dict.pop(key)\n",
        "                    temp_probs_dict.pop(key)\n",
        "\n",
        "            # Append the two n-th dictionaries to the corresponding list of dictionaries\n",
        "            list_of_tfidf_dicts.append(temp_tfidf_dict)\n",
        "            list_of_probs_dicts.append(temp_probs_dict)\n",
        "\n",
        "    elif gluename == 'SCP':\n",
        "\n",
        "        # For each n in [1, len(d)]\n",
        "        for n in range(1, len(d)):\n",
        "\n",
        "            g = dict(d[n - 1])\n",
        "            g2 = dict(d[n - 1])\n",
        "            for keys, value in d[n - 1].items():\n",
        "                if not (n == 1 and len(keys) < 3):\n",
        "                    tfidf_mod = []\n",
        "                    probs = []\n",
        "                    num_non_zero_doc = (len(value) - 1) / 2\n",
        "                    for doc_num in range(1, len(value), 2):\n",
        "                        vec_len_words = []\n",
        "                        for j in str_to_list(keys):\n",
        "                            vec_len_words.append(len(j))\n",
        "                        tfidf_mod.append(value[doc_num])\n",
        "                        tfidf_mod.append(np.mean(vec_len_words) * value[doc_num + 1] * np.log(\n",
        "                            len(corpus) / num_non_zero_doc) / len(corpus[value[doc_num]]))\n",
        "                        probs.append(value[doc_num])\n",
        "                        probs.append(value[doc_num + 1] / len(corpus[value[doc_num]]))\n",
        "\n",
        "                    medprob = sum(probs[1::2]) / len(corpus)\n",
        "                    probs[1::2] = [l1 - medprob for l1 in probs[1::2]]\n",
        "\n",
        "                    num = value[0]\n",
        "                    g[keys] = []\n",
        "                    g2[keys] = []\n",
        "                    if n != 1:\n",
        "                        key = str_to_list(keys)\n",
        "                        somma = 0\n",
        "                        for i in range(len(key) - 1):\n",
        "                            f1 = d[i][list_to_str(key[:i + 1])][0]\n",
        "                            f2 = d[n - i - 2][list_to_str(key[i + 1:])][0]\n",
        "                            somma += (f1*f2) / (n - 1)\n",
        "                        g[keys].append((num**2) / somma)\n",
        "                        g2[keys].append((num**2) / somma)\n",
        "                    for val in tfidf_mod:\n",
        "                        g[keys].append(val)\n",
        "                    for val2 in probs:\n",
        "                        g2[keys].append(val2)\n",
        "                else:\n",
        "                    g.pop(keys)\n",
        "                    g2.pop(keys)\n",
        "\n",
        "            list_of_tfidf_dicts.append(g)\n",
        "            list_of_probs_dicts.append(g2)\n",
        "\n",
        "    elif gluename == 'MI':\n",
        "        for n in range(1, len(d)):\n",
        "            g = dict(d[n - 1])\n",
        "            g2 = dict(d[n - 1])\n",
        "            for keys, value in d[n - 1].items():\n",
        "                if not (n == 1 and len(keys) < 3):\n",
        "                    tfidf_mod = []\n",
        "                    probs = []\n",
        "                    num_non_zero_doc = (len(value) - 1) / 2\n",
        "                    for doc_num in range(1, len(value), 2):\n",
        "                        vec_len_words = []\n",
        "                        for j in str_to_list(keys):\n",
        "                            vec_len_words.append(len(j))\n",
        "                        tfidf_mod.append(value[doc_num])\n",
        "                        tfidf_mod.append(np.mean(vec_len_words) * value[doc_num + 1] * np.log(\n",
        "                            len(corpus) / num_non_zero_doc) / len(corpus[value[doc_num]]))\n",
        "                        probs.append(value[doc_num])\n",
        "                        probs.append(value[doc_num + 1] / len(corpus[value[doc_num]]))\n",
        "\n",
        "                    medprob = sum(probs[1::2]) / len(corpus)\n",
        "                    probs[1::2] = [l1 - medprob for l1 in probs[1::2]]\n",
        "\n",
        "                    num = value[0]\n",
        "                    g[keys] = []\n",
        "                    g2[keys] = []\n",
        "                    if n != 1:\n",
        "                        key = str_to_list(keys)\n",
        "                        somma = 0\n",
        "                        for i in range(len(key) - 1):\n",
        "                            f1 = d[i][list_to_str(key[:i + 1])][0]\n",
        "                            f2 = d[n - i - 2][list_to_str(key[i + 1:])][0]\n",
        "                            somma += (f1*f2) / (n - 1)\n",
        "                        g[keys].append(np.log(num / somma))\n",
        "                        g2[keys].append(np.log(num / somma))\n",
        "                    for val in tfidf_mod:\n",
        "                        g[keys].append(val)\n",
        "                    for val2 in probs:\n",
        "                        g2[keys].append(val2)\n",
        "                else:\n",
        "                    g.pop(keys)\n",
        "                    g2.pop(keys)\n",
        "\n",
        "            list_of_tfidf_dicts.append(g)\n",
        "            list_of_probs_dicts.append(g2)\n",
        "\n",
        "    return list_of_tfidf_dicts, list_of_probs_dicts"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
